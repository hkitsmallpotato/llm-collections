{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7e88740885d421eabe3b1255f3e8cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_944e795c4da947588eb41ffee0e1d872",
              "IPY_MODEL_a799975277c741afb5a778669c3f4372",
              "IPY_MODEL_dd26b0eb3c044cf29589da722abe581f"
            ],
            "layout": "IPY_MODEL_a911029f33ef4388ac15de8ff08c60a3"
          }
        },
        "944e795c4da947588eb41ffee0e1d872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b5b38807cf426fac9e8e9d5999d2e0",
            "placeholder": "​",
            "style": "IPY_MODEL_9a0bf96e322e4fedbb1fa539726423bf",
            "value": "Downloading (…)naco-GGML-q3_K_M.bin: 100%"
          }
        },
        "a799975277c741afb5a778669c3f4372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60c1025798ca4e588b4829c3195e6f65",
            "max": 6249231488,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae526e39959444c487652b44ceb01336",
            "value": 6249231488
          }
        },
        "dd26b0eb3c044cf29589da722abe581f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d43ca61bb6594b3ab208a94f334be53d",
            "placeholder": "​",
            "style": "IPY_MODEL_cc2b79534f6b4779adf9406f12a05ccd",
            "value": " 6.25G/6.25G [02:09&lt;00:00, 44.6MB/s]"
          }
        },
        "a911029f33ef4388ac15de8ff08c60a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b5b38807cf426fac9e8e9d5999d2e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a0bf96e322e4fedbb1fa539726423bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60c1025798ca4e588b4829c3195e6f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae526e39959444c487652b44ceb01336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d43ca61bb6594b3ab208a94f334be53d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc2b79534f6b4779adf9406f12a05ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running Open Source LLM - CPU/GPU-hybrid option via llama.cpp\n",
        "\n",
        "In this tutorial, we will learn how to run open source LLM in a reasonably large range of hardware, even those with low-end GPU only or no GPU at all.\n",
        "\n",
        "Traditionally AI models are trained and run using deep learning library/frameworks such as `tensorflow` (Google), `pytorch` (Meta), `huggingface` etc. Although they can be used directly in production, they are also designed to be used by AI/ML researcher to heavily customize in order to push the Sota (State of the art) forward. As such they carry lots of \"baggage\".\n",
        "\n",
        "This is one of the key insight exploited by the man behind the project of `ggml`, a low level, C reimplementation of just the parts that are actually needed to run inference of transformer based neural network. `llama.cpp` then build on top of this to make it possible to run LLM on CPU only. (The actual history of the project is quite a bit more messy and what you hear is a sanitized version) Later on, they also added ability to partially or fully offload model to GPU, so that one can still enjoy partial acceleration.\n",
        "\n",
        "`llama.cpp` is by itself just a C program - you compile it, then run it from the command line. This is one way to run LLM, but it is also possible to call LLM from inside python using a form of FFI (Foreign Function Interface) - in this case the \"official\" binding recommended is `llama-cpp-python`, and that's what we'll use today."
      ],
      "metadata": {
        "id": "RK3QQJJ8YWL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Running llama.cpp from command line\n",
        "\n",
        "Reference:\n",
        "\n",
        "- [Official llama.cpp repo](https://github.com/ggerganov/llama.cpp)\n",
        "\n",
        "You may skip this subsection, but if you want to most direct experience, you can run the following commands:\n",
        "\n",
        "- Install `llama.cpp` *(note that we go for the absolute minimum installation without any performance enhancement)*:"
      ],
      "metadata": {
        "id": "X2DaVcHRaVdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp && cd llama.cpp && make"
      ],
      "metadata": {
        "id": "h3rSg3HLanFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3bc883-bd56-4417-98d7-998e16daf5b9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 4744, done.\u001b[K\n",
            "remote: Counting objects: 100% (2280/2280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (434/434), done.\u001b[K\n",
            "remote: Total 4744 (delta 2125), reused 1861 (delta 1846), pack-reused 2464\u001b[K\n",
            "Receiving objects: 100% (4744/4744), 3.96 MiB | 14.84 MiB/s, done.\n",
            "Resolving deltas: 100% (3231/3231), done.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c ggml.c -o ggml.o\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c llama.cpp -o llama.o\n",
            "\u001b[01m\u001b[Kllama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_sample_classifier_free_guidance(llama_context*, llama_token_data_array*, llama_context*, float, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kllama.cpp:2190:51:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Koperation on ‘\u001b[01m\u001b[Kt_start_sample_us\u001b[m\u001b[K’ may be undefined [\u001b[01;35m\u001b[K-Wsequence-point\u001b[m\u001b[K]\n",
            " 2190 |     int64_t t_start_sample_us = \u001b[01;35m\u001b[Kt_start_sample_us = ggml_time_us()\u001b[m\u001b[K;\n",
            "      |                                 \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -c examples/common.cpp -o common.o\n",
            "cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS   -c -o k_quants.o k_quants.c\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/main/main.cpp ggml.o llama.o common.o k_quants.o -o main \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize/quantize.cpp ggml.o llama.o k_quants.o -o quantize \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o -o quantize-stats \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o -o perplexity \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o -o embedding \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS pocs/vdot/vdot.cpp ggml.o k_quants.o -o vdot \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o k_quants.o -o train-text-from-scratch \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o -o simple \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o k_quants.o -o server \n",
            "g++ --shared -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o -o libembdinput.so \n",
            "g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o -o embd-input-test  -L. -lembdinput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download model using `aria2c` for robustness *(detailed explanation of how to choose model, quantization level, and prompts format skipped as they're covered in next section) (Also note we need the `-o` flag as HuggingFace uses git LFS for large files, so the link redirect and the filename need to be corrected)*:"
      ],
      "metadata": {
        "id": "4p-gjewgcvr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y aria2"
      ],
      "metadata": {
        "id": "KZNQP6Uxc-mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15821bc0-7e5a-4a62-f2a2-35f1c609b489"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,070 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,866 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,346 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,369 kB]\n",
            "Fetched 8,991 kB in 2s (3,823 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,475 kB of archives.\n",
            "After this operation, 5,959 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libc-ares2 amd64 1.15.0-1ubuntu0.3 [36.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libaria2-0 amd64 1.35.0-1build1 [1,082 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 aria2 amd64 1.35.0-1build1 [356 kB]\n",
            "Fetched 1,475 kB in 0s (3,647 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 123105 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.15.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.15.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.35.0-1build1_amd64.deb ...\n",
            "Unpacking aria2 (1.35.0-1build1) ...\n",
            "Setting up libc-ares2:amd64 (1.15.0-1ubuntu0.3) ...\n",
            "Setting up libaria2-0:amd64 (1.35.0-1build1) ...\n",
            "Setting up aria2 (1.35.0-1build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!aria2c https://huggingface.co/TheBloke/vicuna-13b-v1.3.0-GGML/resolve/main/vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin -o vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin"
      ],
      "metadata": {
        "id": "LphKOQA3dEeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2978f6af-9d0b-45de-ba2d-161c9d356d3e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "07/11 20:59:33 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "07/11 20:59:33 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/6d/58/6d58b4d5ddcda9696a42e991ffa02e907a318665d026fae7962da31e446bba86/4767c77db1b80896b0b6441e784708ddd0d2dc4885e52867603c1da81fee1f36?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin%3B+filename%3D%22vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1689368373&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTM2ODM3M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy82ZC81OC82ZDU4YjRkNWRkY2RhOTY5NmE0MmU5OTFmZmEwMmU5MDdhMzE4NjY1ZDAyNmZhZTc5NjJkYTMxZTQ0NmJiYTg2LzQ3NjdjNzdkYjFiODA4OTZiMGI2NDQxZTc4NDcwOGRkZDBkMmRjNDg4NWU1Mjg2NzYwM2MxZGE4MWZlZTFmMzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=NQPYL%7EmpKWunf2V%7EKldhvBJD28UvL8OPtgsilHtXtwz2DHyzmFFJ%7EL7huJE8w9jfBPrrT5wBl9QouulpgYS2QziidIaOCNE9C-UIMZe%7EHGrffnEGFomjl27R1M0b2RVKr3xHVqoRaU9DI5wrJPJv1IRW387NyU9nnrM8iN0ClNQekgbb1nH7e8wjnTEKlHzK982ZFM6dbnfyDrEsee86kvZhCFvhFsb75QIGDxQBAT4WcwLAK%7EDppKkYMPHFOFAk6r-YJLkBn7ERQG5E74wc4epbFlE1J4a6qP6FjmM%7Eg61REq6BarnHKlRyHF4xvQd6EDKDRVZYTvmykm9cCj-PDA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[0m\n",
            "07/11 21:00:13 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "0f0f40|\u001b[1;32mOK\u001b[0m  |   148MiB/s|/content/vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Run the compiled program in interactive/chat mode (output speed depends on the CPU, but expect 1-5 tokens/sec in general, so please be patient waiting for outputs) (If it prematurely return control, you may use the prompt \"Please Continue.\") (In colab you can enter text by clicking the area with a mouse first):"
      ],
      "metadata": {
        "id": "D5lbB0RldY3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/main -m ./vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin -n 750 --repeat_penalty 1.1 \\\n",
        "--color -i -r \"USER:\" \\\n",
        "-p \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nUSER: What are the steps to create a new website?\\nASSISTANT:\""
      ],
      "metadata": {
        "id": "0EAOAmjEdfk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816b8abf-b0f6-45d2-cf2e-8528ae31fb7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: build = 819 (5bf2a27)\n",
            "main: seed  = 1689105364\n",
            "llama.cpp: loading model from ./vicuna-13b-v1.3.0.ggmlv3.q3_K_M.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32000\n",
            "llama_model_load_internal: n_ctx      = 512\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 12 (mostly Q3_K - Medium)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: mem required  = 8068.43 MB (+ 1608.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  =  400.00 MB\n",
            "\n",
            "system_info: n_threads = 2 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "main: interactive mode on.\n",
            "Reverse prompt: 'USER:'\n",
            "sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 750, n_keep = 0\n",
            "\n",
            "\n",
            "== Running in interactive mode. ==\n",
            " - Press Ctrl+C to interject at any time.\n",
            " - Press Return to return control to LLaMa.\n",
            " - To return control without starting a new line, end your input with '/'.\n",
            " - If you want to submit another line, end your input with '\\'.\n",
            "\n",
            "\u001b[33m A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\n USER: What are the steps to create a new website?\\n ASSISTANT:\u001b[0m Here are the general steps to create a new website:\n",
            "1. Define your website’s purpose and goals.\n",
            "USER:\u001b[1m\u001b[32mPlease continue.\n",
            "\u001b[0m2. Research domain name registration options, including whether you will use a .com, .org, or another type of domain extension.\n",
            "3. Choose a web hosting service that meets your website’s needs in terms of storage, bandwidth, and features.\n",
            "4. Select a content management system (CMS) or programming language for building your website, such as WordPress or HTML/CSS.\n",
            "5. Plan the layout and design of your website, including any necessary graphics and images.\n",
            "6. Write content for your website, including text, images, videos, and other media.\n",
            "7. Test your website to ensure that it is functional and easy to use on different devices and browsers.\n",
            "8. Launch your website and make it live for the public to access.\n",
            "9. Regularly update and maintain your website with new content, design changes, and security updates as needed.\n",
            "USER:\u001b[1m\u001b[32m\u001b[0m\n",
            "\n",
            "llama_print_timings:        load time = 30346.96 ms\n",
            "llama_print_timings:      sample time =   134.21 ms /   210 runs   (    0.64 ms per token,  1564.77 tokens per second)\n",
            "llama_print_timings: prompt eval time = 54105.83 ms /    58 tokens (  932.86 ms per token,     1.07 tokens per second)\n",
            "llama_print_timings:        eval time = 263447.50 ms /   210 runs   ( 1254.51 ms per token,     0.80 tokens per second)\n",
            "llama_print_timings:       total time = 366589.38 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the command line arguments (see also [this](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)):\n",
        "\n",
        "- `-m`: Path to model\n",
        "- `-n`: Max. new number of token generated\n",
        "- `--repeat_penalty`: Repetition penalty\n",
        "- `--color`: Enable color\n",
        "- `-i`: Run in interactive mode\n",
        "- `-r`: Reverse prompt - program will detect presence of this word as a cue to pause generation and pass control back to user\n",
        "- `-p`: Initial prompt\n",
        "\n",
        "In this chat mode, the LLM will generate continuation of the initial prompt until it encounter reverse prompt, then it will be user's turn. After user entered next query, that input will be appended to the total text generated so far and fed to LLM for text continuation again. This process will loop to create a UX that is like a chatbot."
      ],
      "metadata": {
        "id": "32tkMFiweQWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and model download\n",
        "\n",
        "In this section we will assume you have a Nvidia GPU.\n",
        "\n",
        "First we will install the `llama-cpp-python` library:"
      ],
      "metadata": {
        "id": "iHA5F_jfaOH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t5hn5BIoXub4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244fc777-428b-4cc4-c59a-b0d921a3aadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.1.70.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.22.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.70-cp310-cp310-linux_x86_64.whl size=447493 sha256=1beff35070bff0e476fc8093b5d191462b3a56bb04e2410a7087a00c605c260f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/53/7b/bdefa38c9f6817f9ffa6bec0959edc3d17e85e50569156afc7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.70\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command might be confusing so let's explain it. To make it very clear please always keep in mind that `llama.cpp` is the actual library to run LLM inference, and it is a C program; `llama-cpp-python` on the other hand is a python FFI binding to this underlying library.\n",
        "\n",
        "Note the use of the `FORCE_CMAKE=1` ephemeral enviornment variable in the shell to change `pip`'s behavior as the library build the underlying `llama.cpp`, which is vendorized. `llama.cpp` itself can be built with either `make` or `CMake`, but XX supports the various compiler flag to enable builds with GPU support enabled only when using `CMake`, which is why we have to do this.\n",
        "\n",
        "Then we use another ephemeral shell enviornment variable `CMAKE_ARGS` to change the arguments passed to `Cmake`. This part is just following instruction in `llama.cpp`'s installation guide."
      ],
      "metadata": {
        "id": "5nW7-zr-gw-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to download models. `llama.cpp` uses its own specific model formats, using the same name as the base library, GGML. GGML is a minimalistic format to store model weights and also supports its own quantization scheme.\n",
        "\n",
        "### A short guide on choosing models\n",
        "\n",
        "*(Note: this is just an introduction, I have another post that goes into more details)*\n",
        "\n",
        "The open source community has been thriving around fine-tuned LLM produced by enthusiasts. Although the ecosystem is largest for fine-tunes based on the llama foundation models, with dataset produced using model extraction from OpenAI's GPT3.5/4, plus self-instruct methods like in the Alpaca paper, there have been other families too more recently, such as Falcon, MPT, X-Gen, and StarCoder. Choosing a specific model to use involves many factors, and the models have many relevant technical specifications. But generally speaking, one may choose a model suitable for his/her use case based on these steps:\n",
        "\n",
        "1. **Choose a family** - The r/Localllama has a wiki with recommendation, and there are kind of a consensus on the best general purpose choice for the most common situations - Vicuna/Manticore/Guanaco for generic use, WizardLM for complex instruction following, SuperCoT (Super Chain of thought) for LLM based application development use (i.e. agent and langchain etc). There are also suggested models for Story writing and roleplay. You may also look at public leaderboard/evaluation/arena that ranks the model's relative strength, such as one compiled by HuggingFace, or the one at LMSYS.\n",
        "2. **Decide on model size and quantization level** - Generally, the larger (the model size) the better as it will be more intelligent. So you usually would pick the largest one that can fit in memory. Memory requirement can be calculated based on model size plus quantization level, and sometimes you may want to make a trade off of choosing a lower quantization level while switching to larger model for the same memory budget (as experiments have shown this trade off results in better output quality overall, so it is worth it). In general, for very small/edge device, pick 7B (but don't expect to do any complex tasks). Otherwise, for 16GB, pick 33B at q3 for more demanding use case, or 13B at q4.\n",
        "3. **Locate the model on HuggingFace** - Go to the HuggingFace website, and use the search function, entering just the model name, then click \"Show all results\". You will likely see a dazzling list of models with same name but different technical parameters. Based on your choice in previous step, choose the one with the correct parameters, remembering that it should have the keyword \"GGML\" on it and NOT \"GPTQ\" (which is for the *other* option to run LLM). Names with neither of it are the original fp16 model. The user `TheBloke` has a semi-autoamted approach to uploading quantized model and has a reliable model format.\n",
        "\n",
        "### Downloading model with help of huggingface-hub\n",
        "\n",
        "Although we may download models using the website's own UI and the browser's builtin mechanism, or via command line tools, we may also use an official huggingface client library for convinience.\n",
        "\n",
        "The library let us access the HuggingFace API programmatically. Although for our case, we will only be using its function to download models, keep in mind that it can also be used for things like searching for models, uploading your own models/datasets, and so on.\n",
        "\n",
        "There are two approach to download models:\n",
        "\n",
        "- The `hf_hub_download` function let us download specific file, while being able to pin to a specific version too.\n",
        "- The `snapshot_download` function let us download entire repository, as well as a more targeted/flexible download with include/exclude pattern.\n",
        "\n",
        "So let's first install the client:"
      ],
      "metadata": {
        "id": "o2IWs3tOiA2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "lXbWA9rYi4db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce855235-641d-4044-c554-1253c205b150"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/268.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/268.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then download some models:"
      ],
      "metadata": {
        "id": "Dw6bNKhji9Bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "#path = hf_hub_download(repo_id=\"TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML\", filename=\"WizardLM-Uncensored-SuperCOT-Storytelling.ggmlv3.q3_K_S.bin\")\n",
        "path = hf_hub_download(repo_id=\"mindrage/Manticore-13B-Chat-Pyg-Guanaco-GGML\", filename=\"Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin\")"
      ],
      "metadata": {
        "id": "fznuar_zi_kd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a7e88740885d421eabe3b1255f3e8cef",
            "944e795c4da947588eb41ffee0e1d872",
            "a799975277c741afb5a778669c3f4372",
            "dd26b0eb3c044cf29589da722abe581f",
            "a911029f33ef4388ac15de8ff08c60a3",
            "26b5b38807cf426fac9e8e9d5999d2e0",
            "9a0bf96e322e4fedbb1fa539726423bf",
            "60c1025798ca4e588b4829c3195e6f65",
            "ae526e39959444c487652b44ceb01336",
            "d43ca61bb6594b3ab208a94f334be53d",
            "cc2b79534f6b4779adf9406f12a05ccd"
          ]
        },
        "outputId": "4e87dc7b-7285-4117-b1b8-34189304e01a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)naco-GGML-q3_K_M.bin:   0%|          | 0.00/6.25G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e88740885d421eabe3b1255f3e8cef"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMuuY34bsgQq",
        "outputId": "456da0d9-4b28-4be5-d952-1d7a6776863f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--mindrage--Manticore-13B-Chat-Pyg-Guanaco-GGML/snapshots/723b3a9e341c49ead85e28b8606366c19b9a8ff5/Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here `hf_hub_download` let us download specific files from a repo. You can even pin a particular version/tag if necessary. After the download is done it returns a full path to where the file is stored."
      ],
      "metadata": {
        "id": "sVthwPs-jMTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running LLM from python\n",
        "\n",
        "Now let's get to business. First initialize the library, which will detect GPUs:"
      ],
      "metadata": {
        "id": "9IK0r-URji4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "NqIH90ZojqBu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load the models (you may need to change the value of `n_gpu_layers`, see below):"
      ],
      "metadata": {
        "id": "O1YXfupTjrN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(model_path=path, n_ctx=2048, n_gpu_layers=45)"
      ],
      "metadata": {
        "id": "4XcP6yVEjtnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1980c292-883a-41ea-bbba-0dca4ca025b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `n_gpu_layers` argument is useful when you have enabled GPU in your builds. It is the number of layers in the neural network to offload to GPU. 0 means CPU only inference, while the larger this value, the more offloading happen which would hopefully increase speed, but will also consume more VRAM.\n",
        "\n",
        "Be careful that if you set it too large, you may get OOM (Out-of-memory) on your GPU and as this is a C program you may get strange behavior like things hanging or crashing the jupyter kernel. Better to be conservative at first and increase using experiment to estimate VRAM cost per layer to calculate how much you can afford to offload.\n",
        "\n",
        "In the author's test:\n",
        "\n",
        "- (33B/q3 model) Offload 23 layers: 7.1GB ram (main) + 6GB vram (GPU)\n",
        "- (13B/q3 model) Offload all 43 layers: 5.5GB ram (main) + 8GB vram (GPU)\n",
        "\n",
        "*Also note that recent Nvidia driver has a form of aggressive memory offloading where they offload memory back to the main memory, which results in significant speed degradation because LLM is heavily memory and memory-bandwidth bound. Although they've been working on it, the advice at the moment is to stay at an old enough driver version or to downgrade.*"
      ],
      "metadata": {
        "id": "0kVc9Klfj6Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we run a short smoke test:"
      ],
      "metadata": {
        "id": "u_0TPf91lG4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, echo=True)"
      ],
      "metadata": {
        "id": "fdMQtuPMlJsR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a full list of arguments available, please refer to the [manual](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__call__).\n",
        "\n",
        "Note that the underlying `llama.cpp` will output statistics on generation speed to console (but suppressed on Google Colab). The time taken has the following breakdown:\n",
        "\n",
        "- Sample: Time to run the sampling algorithm to select token based on the probability distribution returned by the LLM. (Which may involves modifying the probabilities as a form of post-processing) Usually insignificant.\n",
        "- Prompt eval: Time taken to run the user prompt through the network to generate the internal values, which will be reused in subsequent runs of \"Prompt + partially generated text\" fed through the network.\n",
        "- Eval: Time taken to actually compute the next token predictions by feeding the input (Prompt + partially generated text) through the neural network.\n",
        "\n",
        "Then a token per second stat is computed. Notice however that eval time actually gradually increases as the length of prompt preceeding it increases (since we are appending to the prompt token-by-token essentially). This is related to the **quadratic bottleneck of the attention mechanism**.\n",
        "\n",
        "With the lecturing done, let's check if it worked or not:"
      ],
      "metadata": {
        "id": "XDWmQCW4ltLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "XnDmElptlOv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1154d25f-614e-4685-bb9f-5339a2a894ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-e7d950e2-c133-433c-9d9e-a7e2bd887283',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1689188164,\n",
              " 'model': '/root/.cache/huggingface/hub/models--mindrage--Manticore-13B-Chat-Pyg-Guanaco-GGML/snapshots/723b3a9e341c49ead85e28b8606366c19b9a8ff5/Manticore-13B-Chat-Pyg-Guanaco-GGML-q3_K_M.bin',\n",
              " 'choices': [{'text': 'Q: Name the planets in the solar system? A: 1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uran',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 15, 'completion_tokens': 32, 'total_tokens': 47}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `output` object returned by the library contains details of the text-completion. One things that may be very useful to application developer is it contains meta-info on the reason of stopping - is it because the LLM emited one of the stop token, say, or is it because it has generated up to the maximum number of new tokens we allowed it to? (This is found in `output[\"choices\"][0][\"finish_reason\"]`)\n",
        "\n",
        "Let's show the actual LLM response:"
      ],
      "metadata": {
        "id": "Sbjrkzi-lVPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "F9cDOOYwlTKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9edb274e-0d8f-44aa-ed47-8950f4b99bda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Q: Name the planets in the solar system? A: 1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uran'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's try a more serious query to the LLM, using instruction prompt format:"
      ],
      "metadata": {
        "id": "cMtdZ7NTvCkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_prompt = \"\"\"### Instruction\n",
        "Write an essay on the decline of globalization after 2010, giving reasons for why and situate it in historical context.\n",
        "### Response\n",
        "\"\"\"\n",
        "output2 = llm(instruct_prompt, max_tokens=512, echo=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llifO-b9edML",
        "outputId": "a2649376-236e-4bea-e359-7059c0561faa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed an output `Llama.generate: prefix-match hit`. This refers to the cache (*not* the same as an external cache grated on that saves the full prompt response pair, which is a nice trick to improve system performance at the application level, but is not an intrinsic part of LLM). Instead, this cache is related to how Transformer and text generation works, so that we may partially save and reuse the internal values of the neural network during evaluation. The net effect is that prompt evaluation can mostly be skipped from second call (on the same prompt prefix) onward. (Knowledge of transformer would also let one explain why a prefix match is used)"
      ],
      "metadata": {
        "id": "PTZMssacxOGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output2[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUNQOzHMfT-i",
        "outputId": "a88c2876-55e8-4aa6-9e74-89f3cf4e4795"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction\n",
            " Write an essay on the decline of globalization after 2010, giving reasons for why and situate it in historical context.\n",
            "### Response\n",
            "Globalization has been a hotly debated topic in recent years, with many arguing that it has either stalled or even reversed course since 2010. This decline can be attributed to a number of factors, both economic and political. In this essay, we will examine the reasons for the decline of globalization after 2010, situate it in historical context, and explore the implications for the future of international trade and cooperation.\n",
            "Historical Context:\n",
            "Globalization has a long history, dating back to the earliest trade routes between ancient civilizations. However, it accelerated in the late 20th century with the advent of technology, trade liberalization, and the rise of global supply chains. This period of rapid growth was known as the \"golden age\" of globalization, which lasted from the 1980s until the financial crisis of 2008.\n",
            "Post-2010:\n",
            "The financial crisis of 2008 had a profound impact on globalization, triggering a wave of protectionism and populism that swept across developed economies. This was exacerbated by the slow recovery of the global economy, which led to a rise in inequality and a backlash against free trade.\n",
            "Economic Factors:\n",
            "One of the main reasons for the decline of globalization after 2010 is the slowdown in world trade. According to the World Trade Organization (WTO), trade growth has been sluggish since the financial crisis, with an average annual increase of only 3% between 2010 and 2018. This is due in part to ongoing tensions between the United States and China, as well as the rise of protectionist measures such as tariffs and subsidies.\n",
            "Political Factors:\n",
            "Politics have also played a significant role in the decline of globalization. The rise of populism and nationalism in many developed countries has led to a retreat from international cooperation on issues such as climate change, migration, and trade. This has created a more fragmented and uncertain political landscape that is conducive to protectionist measures.\n",
            "Situation in Developing Countries:\n",
            "The decline of globalization has also had a profound impact on developing countries, which have long relied on international trade and investment for growth and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on, let's briefly explain the concept of **prompt format** and **reverse prompt**. Recall that a foundation model is pretty raw and simply perform *text completion*, where it gives what it believes to be the most *natural* continuation of the text. Then LLM that we actually use are usually instruction or chat fine-tuned, so that they behave more inituitively.\n",
        "\n",
        "In the open source LLM ecosystem, model fine-tune are often focused on one or the other:\n",
        "\n",
        "- If it is *instruct tuned*, then you should prompt it like giving a student a worksheet with tasks to perform. This is also called **Alpaca format**.\n",
        "- If it is *tuned on chats*, then you should first describe the role-playing background, then give a transcript of chat with user and assistant taking turns, each conversational turn on a separate line. This is also called **Vicuna format**.\n",
        "\n",
        "Because this might be confusing to beginner, we have deliberately picked a model that merged multiple fine-tune, so what it've been trained on is diverse enough to work with *both* prompt formats - for the most part you don't need to worry with this part as you'd normally need to for other models.\n",
        "\n",
        "We've seen Reverse prompt in the optional section in raw `llama.cpp`. What it does is to detect the presence of specific words and pause generation upon that. This is mainly useful in a chat setting as the LLM would otherwise generation *both* sides of the conversation by itself."
      ],
      "metadata": {
        "id": "u_BsZsvJEuuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More features: Streaming and Token count\n",
        "\n",
        "This library is a relatively barebone and thin-wrapper over the base library. However, it does provide some features that may be useful for application developers. We briefly cover two examples, leaving you the reader to explore more features by reading the manual.\n",
        "\n",
        "We may enable streaming output with the `stream=True` argument. In such case the `output` object returned will be similar, but now the `text` field will only contain the new token generated, instead of the full text with the original prompt included. We may strip the excess metadata and wrap it into a nice little generator ourself. Note the tradeoff however - the per toekn metadata can be useful to keep track of the generation process for example."
      ],
      "metadata": {
        "id": "fgZutUDo1h5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_llm_stream_naive(prompt, max_token, stopwords):\n",
        "    outputs = llm(prompt, max_tokens=max_token, stop=stopwords, \\\n",
        "                  echo=True, stream=True)\n",
        "    for output in outputs:\n",
        "        tok = output['choices'][0]['text']\n",
        "        yield tok"
      ],
      "metadata": {
        "id": "EPBtDQr32dxP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = \"\"\"You are an AI ASSISTANT who is having a chat with USER.\n",
        "You are helpful and will answer USER to the best of your ability.\n",
        "USER: Write a poem involving the theme of embracing uncertainty in life, based on \"The road not chosen\" but modifying it.\n",
        "ASSISTANT: Sure! \"\"\"\n",
        "\n",
        "output_stream = run_llm_stream_naive(chat_prompt, 512, [\"USER:\"])\n",
        "\n",
        "for token in output_stream:\n",
        "    print(token, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMKRIUhGlKmd",
        "outputId": "17b17229-7feb-4ca3-9699-80fd5f6f3bfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Here's a poem called \"The Path Unknown\" that embraces the uncertainty of life:\n",
            "\n",
            "The road untraveled lies before me,\n",
            "Its course unknown and yet to be seen.\n",
            "I step forward, heart pounding,\n",
            "Into the unknown, with a leap of faith.\n",
            "\n",
            "The trees whisper in the breeze,\n",
            "Of adventures that wait in disguise.\n",
            "I will follow where they lead,\n",
            "And see where the path may take me instead.\n",
            "\n",
            "There will be twists and turns,\n",
            "Ups and downs, and moments of fear.\n",
            "But I will face them all with a courage true,\n",
            "And learn from each experience that is new.\n",
            "\n",
            "The unknown can be scary,\n",
            "But it can also be full of light.\n",
            "I will embrace the uncertainty,\n",
            "And let it guide me to a brighter sight.\n",
            "\n",
            "The path untraveled may be rough,\n",
            "But it will also be full of enough.\n",
            "I will follow my heart and soul,\n",
            "And find the beauty that lies in the unknown. \n",
            "\n",
            "So I take a deep breath, and I step out of my comfort zone,\n",
            "Into the unknown world that awaits me alone.\n",
            "I may not know where it will lead,\n",
            "But I am ready to embrace the journey and all that it entails.\n",
            "\n",
            "The path untraveled is mine to explore,\n",
            "And I will walk it with a smile on my face,\n",
            "Knowing that at the end of the road,\n",
            "There will be more adventures to be found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another common situation for application developer is wanting to feed a chunk of a document into a LLM, for example, to generate a summary while respecting the context length limit. To do so, we need a way to count the number of token of a text/cut off a text at a specific token count. But as we know token is not a one-one correspondence with words and there's no trivial method to count it accurately, other than by running the text through the tokenizer actually.\n",
        "\n",
        "The library does provide function to help us do this:"
      ],
      "metadata": {
        "id": "BXOHPYhJ4wNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia article\n",
        "mytext = \"\"\"Food chemistry is the study of chemical processes and interactions of all biological and non-biological components of foods.[1][2] The biological substances include such items as meat, poultry, lettuce, beer, milk as examples. It is similar to biochemistry in its main components such as carbohydrates, lipids, and protein, but it also includes areas such as water, vitamins, minerals, enzymes, food additives, flavors, and colors. This discipline also encompasses how products change under certain food processing techniques and ways either to enhance or to prevent them from happening. An example of enhancing a process would be to encourage fermentation of dairy products with microorganisms that convert lactose to lactic acid; an example of preventing a process would be stopping the browning on the surface of freshly cut apples using lemon juice or other acidulated water.\n",
        "\n",
        "History of food chemistry\n",
        "The scientific approach to food and nutrition arose with attention to agricultural chemistry in the works of J. G. Wallerius, Humphry Davy, and others. For example, Davy published Elements of Agricultural Chemistry, in a Course of Lectures for the Board of Agriculture (1813) in the United Kingdom which would serve as a foundation for the profession worldwide, going into a fifth edition. Earlier work included that by Carl Wilhelm Scheele, who isolated malic acid from apples in 1785.\n",
        "\n",
        "Some of the findings of Liebig on food chemistry were translated and published by Eben Horsford in Lowell Massachusetts in 1848.[3]\n",
        "\n",
        "In 1874 the Society of Public Analysts was formed, with the aim of applying analytical methods to the benefit of the public.[4] Its early experiments were based on bread, milk and wine.\n",
        "\n",
        "It was also out of concern for the quality of the food supply, mainly food adulteration and contamination issues that would first stem from intentional contamination to later with chemical food additives by the 1950s. The development of colleges and universities worldwide, most notably in the United States, would expand food chemistry as well with research of the dietary substances, most notably the Single-grain experiment during 1907-11. Additional research by Harvey W. Wiley at the United States Department of Agriculture during the late 19th century would play a key factor in the creation of the United States Food and Drug Administration in 1906. The American Chemical Society would establish their Agricultural and Food Chemistry Division in 1908 while the Institute of Food Technologists would establish their Food Chemistry Division in 1995.\n",
        "\n",
        "Food chemistry concepts are often drawn from rheology, theories of transport phenomena, physical and chemical thermodynamics, chemical bonds and interaction forces, quantum mechanics and reaction kinetics, biopolymer science, colloidal interactions, nucleation, glass transitions and freezing/disordered or noncrystalline solids, and thus has Food Physical Chemistry as a foundation area.[5][6]\n",
        "\n",
        "Water in food systems\n",
        "Main article: Water\n",
        "A major component of food is water, which can encompass anywhere from 50% in meat products to 95% in lettuce, cabbage, and tomato products. It is also an excellent place for bacterial growth and food spoilage if it is not properly processed. One way this is measured in food is by water activity which is very important in the shelf life of many foods during processing. One of the keys to food preservation in most instances is reduce the amount of water or alter the water's characteristics to enhance shelf-life. Such methods include dehydration, freezing, and refrigeration[7][8][9][10] This field encompasses the \"physiochemical principles of the reactions and conversions that occur during the manufacture, handling, and storage of foods\".[11]\n",
        "\"\"\"\n",
        "\n",
        "doc_as_tokenlist = llm.tokenize(bytes(mytext, \"utf-8\"))\n",
        "print(len(doc_as_tokenlist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MRyhDOn5XR4",
        "outputId": "5dbb21ae-aea3-417c-9a8c-1ca408476db2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_example = doc_as_tokenlist[150:200]\n",
        "print(llm.detokenize(chunk_example).decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9rk7nfN6ChT",
        "outputId": "eaef6de6-23d7-47e8-d139-94ecbb1b1063"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " would be to encourage fermentation of dairy products with microorganisms that convert lactose to lactic acid; an example of preventing a process would be stopping the browning on the surface of freshly cut apples using le\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Interface for more experimentation\n",
        "\n",
        "So far we've been interacting with LLM through a programmatic interface, or the command line. For heavier experimentations, we may need trial and error on prompts as smallish LLM like the one we're using so far can be sensitive to minute details in how it is prompted in terms of output quality. In such case using an UI, preferrably one that automatically saves the history of request/response/configuration, would seem to make more sense.\n",
        "\n",
        "This part is mostly the same regardless of the backend of how LLM response is actually generated, and we will develop this more fully in the next tutorial where we use the GPU-only option. For now let's get something barebone working:"
      ],
      "metadata": {
        "id": "PxjNt-SX7KqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP7EGfw37QwM",
        "outputId": "3ba8ca7a-37b2-4a0c-8f1b-d2f4cad7828e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-3.36.1-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles (from gradio)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.7 (from gradio)\n",
            "  Downloading gradio_client-0.2.9-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.8/288.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio) (1.22.4)\n",
            "Collecting orjson (from gradio)\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio) (1.10.11)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.14.0)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio) (2.27.1)\n",
            "Collecting semantic-version (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.5.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio) (1.26.16)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio)\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=9196986aacb25d3a4e48cd9193e1f781828ec2151be833dbc36c86c16c120f75\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.100.0 ffmpy-0.3.0 gradio-3.36.1 gradio-client-0.2.9 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "default_prompt = \"\"\"You are an AI ASSISTANT who is having a chat with USER.\n",
        "You are helpful and will answer USER to the best of your ability.\n",
        "USER: Hi! How are you today?\n",
        "ASSISTANT: \"\"\"\n",
        "\n",
        "def submit_llm(prompt, stop, max_token):\n",
        "    text = \"\"\n",
        "    if stop == \"\":\n",
        "        stoplist = []\n",
        "    else:\n",
        "        stoplist = [stop]\n",
        "    output_stream = run_llm_stream_naive(prompt, max_token, stoplist)\n",
        "    # Gradio would substitute your \"yield\" output into the UI directly,\n",
        "    # so we need to apply the append text logic ourself\n",
        "    for token in output_stream:\n",
        "        text = text + token\n",
        "        yield text\n",
        "\n",
        "demo = gr.Interface(submit_llm, \\\n",
        "                    inputs=[gr.TextArea(label=\"Prompt\", value=default_prompt), gr.Textbox(label=\"Reverse prompt\", value=\"USER:\"), gr.Slider(10, 1024, value=512, label=\"Max New Token\")], \\\n",
        "                    outputs=[gr.TextArea(label=\"LLM Response\", show_copy_button=True)])\n",
        "demo.queue()\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "JL-HZnZm8mP0",
        "outputId": "ddc6f8e1-6b14-4a2c-8ce7-61c9c01b9c07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://3035b5604c8655ef91.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3035b5604c8655ef91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Up\n",
        "\n",
        "When we're all done, we may free up the memories with the following code:"
      ],
      "metadata": {
        "id": "y4jN1rRwvYpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import llama_free, llama_free_model"
      ],
      "metadata": {
        "id": "eM2KgDpmvf_G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_free_model(llm.model)"
      ],
      "metadata": {
        "id": "_F3JPexYvmk8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_free(llm.ctx)"
      ],
      "metadata": {
        "id": "Bj-azz71vqE8"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}