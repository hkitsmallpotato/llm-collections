{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning LLM using QLora with axolotl\n",
        "\n",
        "QLora (Quantized Low Rank Adaptor) is a recent technique that made it possible to finetune a LLM with decreased hardware requirement. In this notebook, we're going to do an example run using the `axolotl` tool developed by OpenAccess AI Collective."
      ],
      "metadata": {
        "id": "-9mtuKW6fEj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup"
      ],
      "metadata": {
        "id": "eI9ru6dtfknW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFe-Vh0beQuE",
        "outputId": "ce68ca0a-e2a8-4f98-8bb2-dd0d738d1760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Counting objects: 100% (1997/1997), done.\u001b[K\n",
            "remote: Compressing objects: 100% (433/433), done.\u001b[K\n",
            "remote: Total 4056 (delta 1624), reused 1759 (delta 1472), pack-reused 2059\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 1.58 MiB | 11.48 MiB/s, done.\n",
            "Resolving deltas: 100% (2555/2555), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd axolotl/\n",
        "\n",
        "!pip3 install -e .\n",
        "!pip3 install -U git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR28asoEencY",
        "outputId": "3535ab8f-6faa-4017-ab4b-e85d17793fe4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n",
            "Obtaining file:///content/axolotl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-xzfgiavj/transformers_3c38cf3aea0e44d7a48b6db0d94efbf3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-xzfgiavj/transformers_3c38cf3aea0e44d7a48b6db0d94efbf3\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 05cda5df3405e6a2ee4ecf8f7e1b2300ebda472e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes>=0.39.0 (from axolotl==0.1)\n",
            "  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.21.0 (from axolotl==0.1)\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from axolotl==0.1)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting fire (from axolotl==0.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyYAML==6.0 (from axolotl==0.1)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from axolotl==0.1)\n",
            "  Downloading datasets-2.14.1-py3-none-any.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from axolotl==0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from axolotl==0.1)\n",
            "  Downloading wandb-0.15.7-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from axolotl==0.1)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers (from axolotl==0.1)\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optimum (from axolotl==0.1)\n",
            "  Downloading optimum-1.10.1.tar.gz (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_transfer (from axolotl==0.1)\n",
            "  Downloading hf_transfer-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score==0.3.13 (from axolotl==0.1)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.0 (from axolotl==0.1)\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score==0.1.2 (from axolotl==0.1)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from axolotl==0.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.1) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->axolotl==0.1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->axolotl==0.1) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->axolotl==0.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0->axolotl==0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (3.7.1)\n",
            "Collecting dill (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->axolotl==0.1) (2023.6.0)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.1) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.1) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->axolotl==0.1) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->axolotl==0.1) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->axolotl==0.1) (2.3.0)\n",
            "Collecting coloredlogs (from optimum->axolotl==0.1)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum->axolotl==0.1) (1.11.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl==0.1)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.1)\n",
            "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.1)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->axolotl==0.1)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->axolotl==0.1)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (3.20.3)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers->axolotl==0.1)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers->axolotl==0.1) (4.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (3.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum->axolotl==0.1)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum->axolotl==0.1) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0->axolotl==0.1) (2.1.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: rouge-score, transformers, fire, optimum, pathtools\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=44ff7af1ca4fbb8be287ea7ec5a16fc06b3f643683b827bb08742b8dd5076bd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7447666 sha256=3a2f7dd5e0a2e50065e04a30800b149b5989477ed246c37b817710a1fc6895b6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v1djd6pp/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=8ae97a22570d079aaef4f484b3319c7795dca8d87aa51e8756b0efb2da2e8f0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optimum: filename=optimum-1.10.1-py3-none-any.whl size=358673 sha256=908612d33606efc31cc68b3602445745a987916bf49872017546f24c12f5783c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/2c/05/aa2df11b13d0010ec7c9ce4a9aa6ddb553e4688fb6ba3a50d3\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=6468820bb2972ba6d8e77b3e334e08ca2aa3fa20d80e2a4a519b81d07bf06d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built rouge-score transformers fire optimum pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, bitsandbytes, addict, xxhash, smmap, setproctitle, sentry-sdk, PyYAML, mypy-extensions, humanfriendly, hf_transfer, fire, einops, docker-pycreds, dill, typing-inspect, rouge-score, responses, multiprocess, huggingface-hub, gitdb, coloredlogs, transformers, pyre-extensions, GitPython, wandb, datasets, evaluate, xformers, optimum, bert-score, accelerate, axolotl\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Running setup.py develop for axolotl\n",
            "Successfully installed GitPython-3.1.32 PyYAML-6.0 accelerate-0.21.0 addict-2.4.0 axolotl-0.1 bert-score-0.3.13 bitsandbytes-0.41.0 coloredlogs-15.0.1 datasets-2.14.1 dill-0.3.7 docker-pycreds-0.4.0 einops-0.6.1 evaluate-0.4.0 fire-0.5.0 gitdb-4.0.10 hf_transfer-0.1.3 huggingface-hub-0.16.4 humanfriendly-10.0 multiprocess-0.70.15 mypy-extensions-1.0.0 optimum-1.10.1 pathtools-0.1.2 pyre-extensions-0.0.29 responses-0.18.0 rouge-score-0.1.2 safetensors-0.3.1 sentencepiece-0.1.99 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.32.0.dev0 typing-inspect-0.9.0 wandb-0.15.7 xformers-0.0.20 xxhash-3.3.0\n",
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-kjeibbqt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-kjeibbqt\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 702f9377e3bff179bdef5383f24609d8730ec7c8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (4.32.0.dev0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.5.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0.dev0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.5.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.5.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.5.0.dev0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.5.0.dev0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.5.0.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.5.0.dev0-py3-none-any.whl size=73532 sha256=6c805c443435069bbf4c5251093cfbd6f082dbfc3844bd39d0fd956c0f36d8b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_p4man9h/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.5.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit the config file\n",
        "\n",
        "We'll be running the example at `examples/openllama-3b/qlora.yml`. However, we'll make some modifications:\n",
        "\n",
        "- Some fix to make it work on a T4 GPU (disable `bf16` and `tf32` and enable `fp16`)\n",
        "- Add connections to the wandb tool. It is a SaaS for logging/metric/monitoring of AI training runs, supports uploading checkpoints to save your intermediate work, and more\n",
        "  - Remember to enable periodically saving checkpoints in the first place!\n",
        "\n",
        "Let's examine the content of the config file:"
      ],
      "metadata": {
        "id": "ySq-Wi65fn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat examples/openllama-3b/qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMqmSiNJhGRT",
        "outputId": "9215fa65-9b92-4e18-b887-b849a00fa859"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base_model: openlm-research/open_llama_3b\n",
            "base_model_config: openlm-research/open_llama_3b\n",
            "model_type: LlamaForCausalLM\n",
            "tokenizer_type: LlamaTokenizer\n",
            "load_in_8bit: false\n",
            "load_in_4bit: true\n",
            "strict: false\n",
            "push_dataset_to_hub:\n",
            "datasets:\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len: 2048\n",
            "lora_r: 8\n",
            "lora_alpha: 32\n",
            "lora_dropout: 0.05\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "wandb_project:\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: ./qlora-out\n",
            "batch_size: 4\n",
            "micro_batch_size: 4\n",
            "num_epochs: 2\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: true\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "early_stopping_patience:\n",
            "resume_from_checkpoint:\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 20\n",
            "save_steps:\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.0\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  bos_token: \"<s>\"\n",
            "  eos_token: \"</s>\"\n",
            "  unk_token: \"<unk>\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we should login to wandb:"
      ],
      "metadata": {
        "id": "eQ6qpjdhi4sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbYjVviSjLNh",
        "outputId": "c1945b66-188b-4d52-ea30-8310294eb098"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.7)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.32)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.28.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "_g7eKCfTjC4y",
        "outputId": "6da2fe30-5bf5-4551-a196-eab6c98371fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then make the modification to the config file:"
      ],
      "metadata": {
        "id": "hzPLvBIljNNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEll0PVefBy3",
        "outputId": "7eecbb6d-c358-49b7-a4fb-92832783a038"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Actual edit of config\n",
        "\n",
        "# @markdown Enter your `wandb` project name\n",
        "wandb_project = \"qlora-retest\" # @param {type:\"string\"}\n",
        "# @markdown How often to save checkpoint, default of 50 step takes about 20 minutes per save\n",
        "save_steps = 50 # @param {type:\"integer\"}\n",
        "# @markdown Not tested in this version of notebook yet, enable to use local checkpoint save file and start from there instead of from scratch\n",
        "resume_from_checkpoint = False # @param {type:\"boolean\"}\n",
        "# @markdown Which floating point type to use, requires GPU generation at least Ampere for either `bf16` or `tf32`. For T4, choose `fp16` as the only one that works\n",
        "float_type = \"fp16\" # @param [\"bf16\", \"fp16\", \"tf32\"]\n",
        "\n",
        "\n",
        "import yaml\n",
        "\n",
        "filename = \"examples/openllama-3b/qlora.yml\"\n",
        "\n",
        "with open(filename, \"r\") as file:\n",
        "    qlora_config = yaml.safe_load(file)\n",
        "\n",
        "qlora_config[\"bf16\"] = False\n",
        "qlora_config[\"fp16\"] = False\n",
        "qlora_config[\"tf32\"] = False\n",
        "if float_type == \"bf16\":\n",
        "    qlora_config[\"bf16\"] = True\n",
        "elif float_type == \"fp16\":\n",
        "    qlora_config[\"fp16\"] = True\n",
        "elif float_type == \"tf32\":\n",
        "    qlora_config[\"tf32\"] = True\n",
        "\n",
        "qlora_config[\"wandb_project\"] = wandb_project\n",
        "qlora_config[\"wandb_log_model\"] = \"checkpoint\"\n",
        "qlora_config[\"save_steps\"] = save_steps\n",
        "qlora_config[\"resume_from_checkpoint\"] = resume_from_checkpoint\n",
        "\n",
        "with open(filename, \"w\") as file:\n",
        "    yaml.dump(qlora_config, file)\n",
        "\n",
        "!cat {filename}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nciC7l20gxUk",
        "outputId": "6e056aa9-9feb-481d-ae7e-1edfdee4d585"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter: qlora\n",
            "base_model: openlm-research/open_llama_3b\n",
            "base_model_config: openlm-research/open_llama_3b\n",
            "batch_size: 4\n",
            "bf16: false\n",
            "dataset_prepared_path: last_run_prepared\n",
            "datasets:\n",
            "- path: teknium/GPT4-LLM-Cleaned\n",
            "  type: alpaca\n",
            "debug: null\n",
            "deepspeed: null\n",
            "early_stopping_patience: null\n",
            "eval_steps: 20\n",
            "flash_attention: null\n",
            "fp16: true\n",
            "fsdp: null\n",
            "fsdp_config: null\n",
            "gptq_groupsize: null\n",
            "gptq_model_v1: null\n",
            "gradient_checkpointing: true\n",
            "group_by_length: true\n",
            "learning_rate: 0.0002\n",
            "load_in_4bit: true\n",
            "load_in_8bit: false\n",
            "local_rank: null\n",
            "logging_steps: 1\n",
            "lora_alpha: 32\n",
            "lora_dropout: 0.05\n",
            "lora_fan_in_fan_out: null\n",
            "lora_model_dir: null\n",
            "lora_r: 8\n",
            "lora_target_linear: true\n",
            "lora_target_modules: null\n",
            "lr_scheduler: cosine\n",
            "max_packed_sequence_len: 2048\n",
            "micro_batch_size: 4\n",
            "model_type: LlamaForCausalLM\n",
            "num_epochs: 2\n",
            "optimizer: paged_adamw_32bit\n",
            "output_dir: ./qlora-out\n",
            "push_dataset_to_hub: null\n",
            "resume_from_checkpoint: false\n",
            "save_steps: 50\n",
            "sequence_len: 2048\n",
            "special_tokens:\n",
            "  bos_token: <s>\n",
            "  eos_token: </s>\n",
            "  unk_token: <unk>\n",
            "strict: false\n",
            "tf32: false\n",
            "tokenizer_type: LlamaTokenizer\n",
            "torchdistx_path: null\n",
            "train_on_inputs: false\n",
            "val_set_size: 0.01\n",
            "wandb_log_model: checkpoint\n",
            "wandb_project: qlora-retest\n",
            "wandb_run_id: null\n",
            "wandb_watch: null\n",
            "warmup_steps: 10\n",
            "weight_decay: 0.0\n",
            "xformers_attention: true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's go!\n",
        "\n",
        "(It's going to take ~16 (up to 24?) hours for a complete training run. Since this is just an exercise to prove that it runs, feel free to stop the training in the middle, but please be patient as there will be some delay while it call wandb to sync the data (but there are some extra outputs from wandb that will not be shown in this notebook))\n"
      ],
      "metadata": {
        "id": "LqWyJk2QgK1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch scripts/finetune.py examples/openllama-3b/qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qclPtQXqe0-L",
        "outputId": "5b03d676-31ac-4d7a-f982-b5d7ccaffd3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-30 21:48:19.574197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2023-07-30 21:48:27.132160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2023-07-30 21:48:29,409] [WARNING] [axolotl.validate_config:16] [PID:2744] batch_size is not recommended. Please use gradient_accumulation_steps instead.\n",
            "To calculate the equivalent gradient_accumulation_steps, divide batch_size / micro_batch_size / number of gpus.\n",
            "[2023-07-30 21:48:29,409] [INFO] [axolotl.scripts.train:219] [PID:2744] loading tokenizer... openlm-research/open_llama_3b\n",
            "Downloading tokenizer.model: 100% 534k/534k [00:00<00:00, 12.0MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 330/330 [00:00<00:00, 2.46MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 593/593 [00:00<00:00, 4.56MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565, and set the legacy attribute accordingly.\n",
            "[2023-07-30 21:48:30,042] [DEBUG] [axolotl.load_tokenizer:55] [PID:2744] EOS: 2 / </s>\n",
            "[2023-07-30 21:48:30,042] [DEBUG] [axolotl.load_tokenizer:56] [PID:2744] BOS: 1 / <s>\n",
            "[2023-07-30 21:48:30,042] [DEBUG] [axolotl.load_tokenizer:57] [PID:2744] PAD: None / None\n",
            "[2023-07-30 21:48:30,042] [DEBUG] [axolotl.load_tokenizer:58] [PID:2744] UNK: 0 / <unk>\n",
            "[2023-07-30 21:48:30,043] [INFO] [axolotl.load_tokenized_prepared_datasets:82] [PID:2744] Unable to find prepared dataset in last_run_prepared/c101b4518bc88adecbd8662972d203a9\n",
            "[2023-07-30 21:48:30,043] [INFO] [axolotl.load_tokenized_prepared_datasets:83] [PID:2744] Loading raw datasets...\n",
            "[2023-07-30 21:48:30,043] [INFO] [axolotl.load_tokenized_prepared_datasets:88] [PID:2744] No seed provided, using default seed of 42\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n",
            "Downloading readme: 100% 501/501 [00:00<00:00, 3.73MB/s]\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/36.0M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  12% 4.19M/36.0M [00:00<00:01, 20.0MB/s]\u001b[A\n",
            "Downloading data:  35% 12.6M/36.0M [00:00<00:00, 40.3MB/s]\u001b[A\n",
            "Downloading data:  58% 21.0M/36.0M [00:00<00:00, 47.1MB/s]\u001b[A\n",
            "Downloading data:  82% 29.4M/36.0M [00:00<00:00, 43.9MB/s]\u001b[A\n",
            "Downloading data: 100% 36.0M/36.0M [00:00<00:00, 43.6MB/s]\n",
            "\n",
            "Downloading data:   0% 0.00/4.91M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data: 100% 4.91M/4.91M [00:00<00:00, 33.4MB/s]\n",
            "Downloading data files: 100% 1/1 [00:00<00:00,  1.02it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 737.78it/s]\n",
            "Generating train split: 54568 examples [00:00, 76040.38 examples/s]\n",
            "[2023-07-30 21:48:34,262] [INFO] [axolotl.load_tokenized_prepared_datasets:264] [PID:2744] tokenizing, merging, and shuffling master dataset\n",
            "Map (num_proc=2): 100% 54568/54568 [01:22<00:00, 657.81 examples/s]\n",
            "[2023-07-30 21:50:19,918] [INFO] [axolotl.load_tokenized_prepared_datasets:271] [PID:2744] Saving merged prepared dataset to disk... last_run_prepared/c101b4518bc88adecbd8662972d203a9\n",
            "Saving the dataset (1/1 shards): 100% 54568/54568 [00:00<00:00, 79210.86 examples/s]\n",
            "[2023-07-30 21:50:20,956] [INFO] [axolotl.load_prepare_datasets:364] [PID:2744] packing master dataset to len: 2048\n",
            "[2023-07-30 21:51:05,881] [INFO] [axolotl.load_prepare_datasets:380] [PID:2744] Saving packed prepared dataset to disk... last_run_prepared/fb9fbdc7b5de46a8f80c7d8e96baea30\n",
            "Saving the dataset (1/1 shards): 100% 5528/5528 [00:00<00:00, 26283.70 examples/s]\n",
            "[2023-07-30 21:51:06,113] [INFO] [axolotl.scripts.train:254] [PID:2744] loading model and peft_config...\n",
            "[2023-07-30 21:51:06,245] [INFO] [axolotl.load_model:104] [PID:2744] patching with xformers attention\n",
            "Downloading (…)lve/main/config.json: 100% 506/506 [00:00<00:00, 2.70MB/s]\n",
            "Downloading pytorch_model.bin: 100% 6.85G/6.85G [01:10<00:00, 97.2MB/s]\n",
            "Downloading (…)neration_config.json: 100% 137/137 [00:00<00:00, 91.0kB/s]\n",
            "[2023-07-30 21:53:13,344] [WARNING] [axolotl.load_model:316] [PID:2744] increasing model.config.max_position_embeddings to 2048\n",
            "[2023-07-30 21:53:13,344] [INFO] [axolotl.load_model:325] [PID:2744] converting PEFT model w/ prepare_model_for_kbit_training\n",
            "[2023-07-30 21:53:13,363] [INFO] [axolotl.load_lora:444] [PID:2744] found linear modules: ['o_proj', 'up_proj', 'v_proj', 'q_proj', 'k_proj', 'down_proj', 'gate_proj']\n",
            "trainable params: 12,712,960 || all params: 3,439,186,560 || trainable%: 0.36965020007521776\n",
            "[2023-07-30 21:53:43,052] [INFO] [axolotl.scripts.train:294] [PID:2744] Compiling torch model\n",
            "[2023-07-30 21:53:43,275] [INFO] [axolotl.scripts.train:299] [PID:2744] Pre-saving adapter config to ./qlora-out\n",
            "[2023-07-30 21:53:43,276] [INFO] [axolotl.scripts.train:315] [PID:2744] Starting trainer...\n",
            "[2023-07-30 21:53:43,276] [INFO] [axolotl.scripts.train:317] [PID:2744] hang tight... sorting dataset for group_by_length\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlemontea-tom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/axolotl/wandb/run-20230730_215357-o71am6d5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcurious-water-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lemontea-tom/qlora-retest\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lemontea-tom/qlora-retest/runs/o71am6d5\u001b[0m\n",
            "{'loss': 1.2695, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
            "{'loss': 1.4688, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
            "{'loss': 1.2927, 'learning_rate': 6e-05, 'epoch': 0.0}\n",
            "{'loss': 1.2911, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
            "{'loss': 1.1655, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
            "{'loss': 1.3511, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
            "{'loss': 1.2358, 'learning_rate': 0.00014, 'epoch': 0.01}\n",
            "{'loss': 1.287, 'learning_rate': 0.00016, 'epoch': 0.01}\n",
            "{'loss': 1.1919, 'learning_rate': 0.00018, 'epoch': 0.01}\n",
            "{'loss': 1.3329, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
            "{'loss': 1.0765, 'learning_rate': 0.00019999993359236733, 'epoch': 0.01}\n",
            "{'loss': 1.2112, 'learning_rate': 0.00019999973436955748, 'epoch': 0.01}\n",
            "{'loss': 1.2021, 'learning_rate': 0.00019999940233183508, 'epoch': 0.01}\n",
            "{'loss': 1.306, 'learning_rate': 0.0001999989374796411, 'epoch': 0.01}\n",
            "{'loss': 1.2094, 'learning_rate': 0.00019999833981359296, 'epoch': 0.01}\n",
            "{'loss': 1.3033, 'learning_rate': 0.00019999760933448442, 'epoch': 0.01}\n",
            "{'loss': 1.2345, 'learning_rate': 0.00019999674604328566, 'epoch': 0.01}\n",
            "{'loss': 1.1763, 'learning_rate': 0.00019999574994114335, 'epoch': 0.01}\n",
            "{'loss': 1.2333, 'learning_rate': 0.00019999462102938037, 'epoch': 0.01}\n",
            "{'loss': 1.213, 'learning_rate': 0.00019999335930949612, 'epoch': 0.01}\n",
            "  1% 20/2736 [07:01<15:46:41, 20.91s/it]\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:05<00:33,  2.77s/it]\u001b[A\n",
            " 21% 3/14 [00:11<00:43,  3.95s/it]\u001b[A\n",
            " 29% 4/14 [00:16<00:45,  4.52s/it]\u001b[A\n",
            " 36% 5/14 [00:22<00:43,  4.85s/it]\u001b[A\n",
            " 43% 6/14 [00:27<00:40,  5.10s/it]\u001b[A\n",
            " 50% 7/14 [00:33<00:36,  5.25s/it]\u001b[A\n",
            " 57% 8/14 [00:38<00:31,  5.21s/it]\u001b[A\n",
            " 64% 9/14 [00:43<00:26,  5.32s/it]\u001b[A\n",
            " 71% 10/14 [00:49<00:21,  5.35s/it]\u001b[A\n",
            " 79% 11/14 [00:54<00:15,  5.28s/it]\u001b[A\n",
            " 86% 12/14 [00:59<00:10,  5.34s/it]\u001b[A\n",
            " 93% 13/14 [01:05<00:05,  5.38s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.181932806968689, 'eval_runtime': 76.4827, 'eval_samples_per_second': 0.732, 'eval_steps_per_second': 0.183, 'epoch': 0.01}\n",
            "  1% 20/2736 [08:17<15:46:41, 20.91s/it]\n",
            "100% 14/14 [01:11<00:00,  5.43s/it]\u001b[A\n",
            "{'loss': 1.1985, 'learning_rate': 0.00019999196478316637, 'epoch': 0.02}\n",
            "{'loss': 1.2438, 'learning_rate': 0.00019999043745224323, 'epoch': 0.02}\n",
            "{'loss': 1.0714, 'learning_rate': 0.00019998877731875524, 'epoch': 0.02}\n",
            "{'loss': 1.2336, 'learning_rate': 0.00019998698438490736, 'epoch': 0.02}\n",
            "{'loss': 1.1357, 'learning_rate': 0.00019998505865308084, 'epoch': 0.02}\n",
            "{'loss': 1.1176, 'learning_rate': 0.00019998300012583333, 'epoch': 0.02}\n",
            "{'loss': 1.2537, 'learning_rate': 0.0001999808088058989, 'epoch': 0.02}\n",
            "{'loss': 1.2098, 'learning_rate': 0.0001999784846961879, 'epoch': 0.02}\n",
            "{'loss': 1.3226, 'learning_rate': 0.0001999760277997872, 'epoch': 0.02}\n",
            "{'loss': 1.271, 'learning_rate': 0.00019997343811995984, 'epoch': 0.02}\n",
            "{'loss': 1.1423, 'learning_rate': 0.00019997071566014535, 'epoch': 0.02}\n",
            "{'loss': 1.1422, 'learning_rate': 0.0001999678604239596, 'epoch': 0.02}\n",
            "{'loss': 1.2029, 'learning_rate': 0.00019996487241519473, 'epoch': 0.02}\n",
            "{'loss': 1.1305, 'learning_rate': 0.0001999617516378193, 'epoch': 0.02}\n",
            "{'loss': 1.3264, 'learning_rate': 0.00019995849809597814, 'epoch': 0.03}\n",
            "{'loss': 1.2266, 'learning_rate': 0.0001999551117939925, 'epoch': 0.03}\n",
            "{'loss': 1.1214, 'learning_rate': 0.0001999515927363599, 'epoch': 0.03}\n",
            "{'loss': 1.2081, 'learning_rate': 0.00019994794092775418, 'epoch': 0.03}\n",
            "{'loss': 1.2039, 'learning_rate': 0.00019994415637302547, 'epoch': 0.03}\n",
            "{'loss': 1.2026, 'learning_rate': 0.00019994023907720027, 'epoch': 0.03}\n",
            "  1% 40/2736 [14:55<14:20:11, 19.14s/it]\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:05<00:33,  2.78s/it]\u001b[A\n",
            " 21% 3/14 [00:11<00:43,  3.95s/it]\u001b[A\n",
            " 29% 4/14 [00:16<00:45,  4.53s/it]\u001b[A\n",
            " 36% 5/14 [00:22<00:43,  4.87s/it]\u001b[A\n",
            " 43% 6/14 [00:27<00:40,  5.11s/it]\u001b[A\n",
            " 50% 7/14 [00:33<00:36,  5.26s/it]\u001b[A\n",
            " 57% 8/14 [00:38<00:31,  5.22s/it]\u001b[A\n",
            " 64% 9/14 [00:44<00:26,  5.34s/it]\u001b[A\n",
            " 71% 10/14 [00:49<00:21,  5.36s/it]\u001b[A\n",
            " 79% 11/14 [00:54<00:15,  5.29s/it]\u001b[A\n",
            " 86% 12/14 [01:00<00:10,  5.36s/it]\u001b[A\n",
            " 93% 13/14 [01:05<00:05,  5.39s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.152363896369934, 'eval_runtime': 76.6763, 'eval_samples_per_second': 0.73, 'eval_steps_per_second': 0.183, 'epoch': 0.03}\n",
            "  1% 40/2736 [16:11<14:20:11, 19.14s/it]\n",
            "100% 14/14 [01:11<00:00,  5.44s/it]\u001b[A\n",
            "{'loss': 1.2681, 'learning_rate': 0.00019993618904548131, 'epoch': 0.03}\n",
            "{'loss': 1.0463, 'learning_rate': 0.0001999320062832477, 'epoch': 0.03}\n",
            "{'loss': 1.2364, 'learning_rate': 0.00019992769079605477, 'epoch': 0.03}\n",
            "{'loss': 1.1188, 'learning_rate': 0.00019992324258963413, 'epoch': 0.03}\n",
            "{'loss': 1.2932, 'learning_rate': 0.00019991866166989367, 'epoch': 0.03}\n",
            "{'loss': 1.1784, 'learning_rate': 0.00019991394804291758, 'epoch': 0.03}\n",
            "{'loss': 1.2541, 'learning_rate': 0.00019990910171496627, 'epoch': 0.03}\n",
            "{'loss': 1.1132, 'learning_rate': 0.0001999041226924764, 'epoch': 0.04}\n",
            "{'loss': 1.1555, 'learning_rate': 0.00019989901098206082, 'epoch': 0.04}\n",
            "{'loss': 1.3394, 'learning_rate': 0.00019989376659050877, 'epoch': 0.04}\n",
            "  2% 50/2736 [19:09<13:35:18, 18.21s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-50)... Done. 0.5s\n",
            "{'loss': 1.2951, 'learning_rate': 0.0001998883895247855, 'epoch': 0.04}\n",
            "{'loss': 1.2399, 'learning_rate': 0.00019988287979203265, 'epoch': 0.04}\n",
            "{'loss': 1.1664, 'learning_rate': 0.0001998772373995679, 'epoch': 0.04}\n",
            "{'loss': 1.117, 'learning_rate': 0.0001998714623548853, 'epoch': 0.04}\n",
            "{'loss': 1.1931, 'learning_rate': 0.00019986555466565493, 'epoch': 0.04}\n",
            "{'loss': 1.2058, 'learning_rate': 0.00019985951433972314, 'epoch': 0.04}\n",
            "{'loss': 1.1864, 'learning_rate': 0.00019985334138511237, 'epoch': 0.04}\n",
            "{'loss': 1.2206, 'learning_rate': 0.0001998470358100213, 'epoch': 0.04}\n",
            "{'loss': 1.0549, 'learning_rate': 0.00019984059762282467, 'epoch': 0.04}\n",
            "{'loss': 1.0102, 'learning_rate': 0.00019983402683207332, 'epoch': 0.04}\n",
            "  2% 60/2736 [22:46<15:50:30, 21.31s/it]\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:05<00:33,  2.78s/it]\u001b[A\n",
            " 21% 3/14 [00:11<00:43,  3.95s/it]\u001b[A\n",
            " 29% 4/14 [00:16<00:45,  4.53s/it]\u001b[A\n",
            " 36% 5/14 [00:22<00:43,  4.86s/it]\u001b[A\n",
            " 43% 6/14 [00:27<00:40,  5.11s/it]\u001b[A\n",
            " 50% 7/14 [00:33<00:36,  5.25s/it]\u001b[A\n",
            " 57% 8/14 [00:38<00:31,  5.23s/it]\u001b[A\n",
            " 64% 9/14 [00:44<00:26,  5.34s/it]\u001b[A\n",
            " 71% 10/14 [00:49<00:21,  5.35s/it]\u001b[A\n",
            " 79% 11/14 [00:54<00:15,  5.30s/it]\u001b[A\n",
            " 86% 12/14 [01:00<00:10,  5.36s/it]\u001b[A\n",
            " 93% 13/14 [01:05<00:05,  5.40s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.1410892009735107, 'eval_runtime': 76.6475, 'eval_samples_per_second': 0.731, 'eval_steps_per_second': 0.183, 'epoch': 0.04}\n",
            "  2% 60/2736 [24:02<15:50:30, 21.31s/it]\n",
            "100% 14/14 [01:11<00:00,  5.44s/it]\u001b[A\n",
            "{'loss': 1.0848, 'learning_rate': 0.00019982732344649433, 'epoch': 0.04}\n",
            "{'loss': 1.3566, 'learning_rate': 0.00019982048747499081, 'epoch': 0.05}\n",
            "{'loss': 1.2252, 'learning_rate': 0.00019981351892664194, 'epoch': 0.05}\n",
            "{'loss': 1.2283, 'learning_rate': 0.00019980641781070307, 'epoch': 0.05}\n",
            "{'loss': 1.1728, 'learning_rate': 0.00019979918413660553, 'epoch': 0.05}\n",
            "{'loss': 1.0926, 'learning_rate': 0.00019979181791395672, 'epoch': 0.05}\n",
            "{'loss': 1.1868, 'learning_rate': 0.00019978431915254017, 'epoch': 0.05}\n",
            "{'loss': 1.0868, 'learning_rate': 0.00019977668786231534, 'epoch': 0.05}\n",
            "{'loss': 1.1116, 'learning_rate': 0.00019976892405341773, 'epoch': 0.05}\n",
            "{'loss': 1.1881, 'learning_rate': 0.00019976102773615892, 'epoch': 0.05}\n",
            "{'loss': 1.1795, 'learning_rate': 0.00019975299892102636, 'epoch': 0.05}\n",
            "{'loss': 1.1028, 'learning_rate': 0.00019974483761868358, 'epoch': 0.05}\n",
            "{'loss': 0.9675, 'learning_rate': 0.00019973654383997007, 'epoch': 0.05}\n",
            "{'loss': 1.2491, 'learning_rate': 0.00019972811759590118, 'epoch': 0.05}\n",
            "{'loss': 1.1393, 'learning_rate': 0.00019971955889766825, 'epoch': 0.05}\n",
            "{'loss': 1.1288, 'learning_rate': 0.00019971086775663857, 'epoch': 0.06}\n",
            "{'loss': 1.1794, 'learning_rate': 0.00019970204418435526, 'epoch': 0.06}\n",
            "{'loss': 0.9819, 'learning_rate': 0.0001996930881925374, 'epoch': 0.06}\n",
            "{'loss': 1.1291, 'learning_rate': 0.0001996839997930799, 'epoch': 0.06}\n",
            "{'loss': 1.2502, 'learning_rate': 0.0001996747789980536, 'epoch': 0.06}\n",
            "  3% 80/2736 [30:57<14:57:43, 20.28s/it]\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:05<00:33,  2.77s/it]\u001b[A\n",
            " 21% 3/14 [00:11<00:43,  3.94s/it]\u001b[A\n",
            " 29% 4/14 [00:16<00:45,  4.52s/it]\u001b[A\n",
            " 36% 5/14 [00:22<00:43,  4.86s/it]\u001b[A\n",
            " 43% 6/14 [00:27<00:40,  5.11s/it]\u001b[A\n",
            " 50% 7/14 [00:33<00:36,  5.25s/it]\u001b[A\n",
            " 57% 8/14 [00:38<00:31,  5.22s/it]\u001b[A\n",
            " 64% 9/14 [00:44<00:26,  5.33s/it]\u001b[A\n",
            " 71% 10/14 [00:49<00:21,  5.35s/it]\u001b[A\n",
            " 79% 11/14 [00:54<00:15,  5.29s/it]\u001b[A\n",
            " 86% 12/14 [01:00<00:10,  5.35s/it]\u001b[A\n",
            " 93% 13/14 [01:05<00:05,  5.39s/it]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.1336756944656372, 'eval_runtime': 76.5931, 'eval_samples_per_second': 0.731, 'eval_steps_per_second': 0.183, 'epoch': 0.06}\n",
            "  3% 80/2736 [32:14<14:57:43, 20.28s/it]\n",
            "100% 14/14 [01:11<00:00,  5.45s/it]\u001b[A\n",
            "{'loss': 1.0629, 'learning_rate': 0.000199665425819705, 'epoch': 0.06}\n",
            "{'loss': 1.0234, 'learning_rate': 0.00019965594027045665, 'epoch': 0.06}\n",
            "{'loss': 1.1065, 'learning_rate': 0.00019964632236290681, 'epoch': 0.06}\n",
            "{'loss': 1.1702, 'learning_rate': 0.00019963657210982948, 'epoch': 0.06}\n",
            "{'loss': 1.2096, 'learning_rate': 0.0001996266895241745, 'epoch': 0.06}\n",
            "{'loss': 1.232, 'learning_rate': 0.00019961667461906743, 'epoch': 0.06}\n",
            "{'loss': 1.0296, 'learning_rate': 0.00019960652740780966, 'epoch': 0.06}\n",
            "{'loss': 1.1686, 'learning_rate': 0.0001995962479038782, 'epoch': 0.06}\n",
            "{'loss': 1.1196, 'learning_rate': 0.00019958583612092576, 'epoch': 0.07}\n",
            "{'loss': 1.3479, 'learning_rate': 0.00019957529207278082, 'epoch': 0.07}\n",
            "{'loss': 1.1783, 'learning_rate': 0.0001995646157734475, 'epoch': 0.07}\n",
            "{'loss': 1.2225, 'learning_rate': 0.0001995538072371055, 'epoch': 0.07}\n",
            "{'loss': 1.0859, 'learning_rate': 0.00019954286647811027, 'epoch': 0.07}\n",
            "{'loss': 1.0763, 'learning_rate': 0.00019953179351099275, 'epoch': 0.07}\n",
            "{'loss': 1.2688, 'learning_rate': 0.00019952058835045957, 'epoch': 0.07}\n",
            "{'loss': 1.1738, 'learning_rate': 0.0001995092510113929, 'epoch': 0.07}\n",
            "{'loss': 1.1326, 'learning_rate': 0.00019949778150885042, 'epoch': 0.07}\n",
            "{'loss': 1.1722, 'learning_rate': 0.0001994861798580654, 'epoch': 0.07}\n",
            "{'loss': 1.2148, 'learning_rate': 0.0001994744460744466, 'epoch': 0.07}\n",
            "{'loss': 1.2499, 'learning_rate': 0.00019946258017357828, 'epoch': 0.07}\n",
            "  4% 100/2736 [38:24<12:27:29, 17.01s/it]\n",
            "  0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            " 14% 2/14 [00:05<00:33,  2.77s/it]\u001b[A\n",
            " 21% 3/14 [00:11<00:43,  3.94s/it]\u001b[A\n",
            " 29% 4/14 [00:16<00:45,  4.52s/it]\u001b[A\n",
            " 36% 5/14 [00:22<00:43,  4.86s/it]\u001b[A\n",
            " 43% 6/14 [00:27<00:40,  5.10s/it]\u001b[A\n",
            " 50% 7/14 [00:33<00:36,  5.25s/it]\u001b[A\n",
            " 57% 8/14 [00:38<00:31,  5.22s/it]\u001b[A\n",
            " 64% 9/14 [00:43<00:26,  5.33s/it]\u001b[A\n",
            " 71% 10/14 [00:49<00:21,  5.35s/it]\u001b[A\n",
            " 79% 11/14 [00:54<00:15,  5.29s/it]\u001b[A\n",
            " 86% 12/14 [01:00<00:10,  5.35s/it]\u001b[A\n",
            " 93% 13/14 [01:05<00:05,  5.39s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 1.1291786432266235, 'eval_runtime': 76.5934, 'eval_samples_per_second': 0.731, 'eval_steps_per_second': 0.183, 'epoch': 0.07}\n",
            "  4% 100/2736 [39:40<12:27:29, 17.01s/it]\n",
            "100% 14/14 [01:11<00:00,  5.45s/it]\u001b[A\n",
            "                                   \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-100)... Done. 5.8s\n",
            "{'loss': 1.1765, 'learning_rate': 0.00019945058217122016, 'epoch': 0.07}\n",
            "{'loss': 1.2101, 'learning_rate': 0.00019943845208330742, 'epoch': 0.07}\n",
            "{'loss': 1.1028, 'learning_rate': 0.0001994261899259507, 'epoch': 0.08}\n",
            "{'loss': 1.2198, 'learning_rate': 0.00019941379571543596, 'epoch': 0.08}\n",
            "{'loss': 1.2474, 'learning_rate': 0.00019940126946822465, 'epoch': 0.08}\n",
            "  4% 105/2736 [41:36<19:39:56, 26.91s/it]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1207, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1941, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1899, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 979, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 625, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1220, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1935, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "  4% 105/2736 [41:41<17:24:34, 23.82s/it]\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}