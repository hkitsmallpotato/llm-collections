{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ePZZWi-aRUL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Merge QLora and Quantize Model\n",
        "\n",
        "In this tutorial, we will take the QLora model checkpoint you've trained from the last notebook, and perform the post processings necessary for it to actually be used. This usually consists of these steps:\n",
        "\n",
        "- Merge the QLora adaptors weight with the base model\n",
        "- Quantize into these formats:\n",
        "  - GPTQ (for GPU)\n",
        "  - GGML (mainly for CPU)\n",
        "- Push to Huggingface\n",
        "\n",
        "You'd need both a wandb and huggingface API key/token."
      ],
      "metadata": {
        "id": "kNLyfZWiUFCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download artifacts from wandb\n",
        "\n",
        "First let's login to wandb as usual."
      ],
      "metadata": {
        "id": "IiV5UXGkUzlr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VV39EafTs08"
      },
      "outputs": [],
      "source": [
        "!pip install wandb huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "_HJbhyupU8zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now find the artifact ID in your wandb dashboard and update the field accordingly:"
      ],
      "metadata": {
        "id": "sEwIWHptU9lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download artifact\n",
        "wandb_artifact_id = \"\" # @param {type:\"string\"}\n",
        "\n",
        "run = wandb.init()\n",
        "artifact = run.use_artifact(wandb_artifact_id, type='model')\n",
        "artifact_dir = artifact.download()\n",
        "print(artifact_dir)\n",
        "!ls {artifact_dir}"
      ],
      "metadata": {
        "id": "VydLL_0SVJuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've downloaded the artifact/checkpoint to a local directory as shown above."
      ],
      "metadata": {
        "id": "qoSmmXjnWiKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge QLora with base model\n",
        "\n",
        "Now, let's begin our work. First login to Huggingface (you may skip this if you don't want to publish, but then you'd need to download the results manually, or upload to your own private storage such as S3 yourself)"
      ],
      "metadata": {
        "id": "CJQwt38ZVyIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "c7vuBHUDWXrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the libraries:"
      ],
      "metadata": {
        "id": "Woqf_5JRW97u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch peft transformers"
      ],
      "metadata": {
        "id": "FIo-rzpkV6m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "The method to perform the merge is to use the Vanilla `transformers` library from Huggingface combined with their `peft` library (Stands for \"Parameter Efficient Fine-tuning\" which feature various methods with the general theme of requiring less trainable parameter than the full model). The reason is because when we trained using the `axolotl` tool, it is actually calling `peft` under the hood and the resulting weights are in their format."
      ],
      "metadata": {
        "id": "zPb596xlXFFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "# Load the PEFT config in our checkpoint\n",
        "config = PeftConfig.from_pretrained(artifact_dir)\n",
        "\n",
        "# Load the base model\n",
        "base_model = LlamaForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    torch_dtype='auto',\n",
        "    device_map='cpu',\n",
        "    use_safetensors=True\n",
        "    # Hopefully you saved the checkpoint as safetensor already,\n",
        "    # as conversion takes additional RAM\n",
        "    # offload_folder=\"offload\", offload_state_dict = True\n",
        "    # (Warning: offloading to harddrive results in some issue when I test)\n",
        ")\n",
        "#tokenizer = LlamaTokenizer.from_pretrained(config.base_model_name_or_path)"
      ],
      "metadata": {
        "id": "A4-TrF8hV7U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "0UhXv5xZYNrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a combined PEFT model using the base model + our checkpoint\n",
        "\n",
        "#model = PeftModel.from_pretrained(base_model, artifact_dir, offload_folder=\"offload\", offload_state_dict = True)\n",
        "# (similar warning, I can't get it to work, would have been nice as it would lessen main memory requirement)\n",
        "model = PeftModel.from_pretrained(base_model, artifact_dir)"
      ],
      "metadata": {
        "id": "-zmzzfOCYOtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And here's where the actual merging occur using magic:**"
      ],
      "metadata": {
        "id": "00BgiSQ5YmDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "lnMw_szcYjnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model"
      ],
      "metadata": {
        "id": "QdAuVKNOYrVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the results to disk. The function interface they provide would do the local save + publish to Huggingface Hub in one step. Though it is also possible to do it separately if you wish."
      ],
      "metadata": {
        "id": "Kjr8e7lVYyWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save model and push\n",
        "fp16_model_save_dir = \"merged_model\" # @param {type:\"string\"}\n",
        "push_to_hub = True # @param {type:\"boolean\"}\n",
        "fp16_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-merged\" # @param {type:\"string\"}\n",
        "\n",
        "merged_model.save_pretrained(fp16_model_save_dir,\n",
        "                             safe_serialization=True,\n",
        "                             push_to_hub=push_to_hub,\n",
        "                             repo_id=fp16_model_repo_id)\n",
        "\n",
        "#merged_model.push_to_hub(fp16_model_repo_id)"
      ],
      "metadata": {
        "id": "zmhSwwmqY_PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now you need to restart the kernel as there seems to be no clean way to reclaim the memory**\n",
        "\n",
        "After that run the cell below to remember the model repo id, which we'll need to use later.\n",
        "\n",
        "You should also login to huggingface hub again if you want to push the models and you have reset the VM instead of just stopping it. (The difference is whether the harddisk is wiped as it stores your token)\n"
      ],
      "metadata": {
        "id": "j-yhgN4-g7c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-merged\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "UqyzWuvAhD48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional sections\n",
        "\n",
        "The procedures in this section is not strictly necessary, but it's nice to do them."
      ],
      "metadata": {
        "id": "6ePZZWi-aRUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Push also the QLora delta weights\n",
        "\n",
        "We will use the API client provided by Huggingface to upload the whole folder directly."
      ],
      "metadata": {
        "id": "5ZAE-xClchOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "qlora_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-peft\" # @param{type:\"string\"}\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=artifact_dir,\n",
        "    repo_id=qlora_model_repo_id,\n",
        "    repo_type=\"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "LhDBpzOmaxWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the merged model\n",
        "\n",
        "Let's try to run inference on the merged model with the original `transformer` library (i.e. most likely in fp16 or even fp32 mode with memory requirement being x2/x4 of 3GB for a 3B param model. You can try the `load_in_4bits` with the `bitsandbytes` library, though that require a GPU)\n",
        "\n"
      ],
      "metadata": {
        "id": "UTrTzrS1ax-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece bitsandbytes"
      ],
      "metadata": {
        "id": "HJ7S77IJbW-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Notice this time we do enable offload_folder to avoid OOM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b\", use_fast=False) #fast tokenizer cause problem\n",
        "model = AutoModelForCausalLM.from_pretrained(fp16_model_repo_id,\n",
        "                                             #torch_dtype=torch.float16,\n",
        "                                             torch_dtype='auto',\n",
        "                                             device_map=\"auto\",\n",
        "                                             offload_folder=\"offload\", offload_state_dict = True)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "YQmI7VWDbawX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Below is a conversation between user and assistent. The assistent is helpful and skillful.\n",
        "User: Hi! How are you today?\n",
        "Assistent: I'm feeling good! Anything I may help you with?\n",
        "User: Write a short essay that analyze ritual from a cultural and anthropology framework.\n",
        "Assistent: Sure! \"\"\"\n",
        "\n",
        "previous_answer = \"\"\" Let's start with the definition of ritual. According to anthropologists, ritual is a set of actions that are performed in a prescribed manner, usually with the intention of achieving a desired outcome. Rituals are often associated with religious or spiritual practices, but they can also be used in non-religious contexts, such as weddings, funerals, or celebrations.\n",
        "In a cultural context, rituals are often seen as a way for people to connect with their traditions, beliefs, and values. They can help to reinforce social norms, transmit knowledge, and transmit values. Rituals can also serve as a way for people to express their identity and their place in the community.\n",
        "Anthropologists believe that rituals are a way for people to make sense of the world around them. They can help to make sense of the past, present, and future, and to make sense of the relationships between people and the natural world. Rituals can also help to create a sense of community and belonging, and to reinforce social norms and values.\n",
        "In conclusion, rituals are a powerful tool for people to connect with their traditions, beliefs, and values, and to make sense of the world around them. They can help to create a sense of community and belonging, and to reinforce social norms and values.\"\"\"\n",
        "\n",
        "followup_question = \"\"\"User: Thanks. I heard that primitive tribes throughout the world have diversity in their rituals, and that some tribes are harsher than others in terms of the rite of passage for their members. Postulate some possible factors explaining this variance.\n",
        "Assistent: \"\"\"\n",
        "\n",
        "response = pipe(prompt, max_new_tokens = 512)\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "sB8-EgoXb15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTQ Quantization\n",
        "\n",
        "We will use the `auto-gptq` library for this task, which is also one of the options for inference in GPU only mode (until `exllama` is released which feature even faster inference speed).\n",
        "\n",
        "This library's author just returned from a break, so expect things to be in more flux.\n",
        "\n",
        "During testing, I found several traps to avoid/pre-empt:\n",
        "\n",
        "- The locale may get messed up (use the quickfix below), which may cause error when attempting to run cells in notebook\n",
        "- Raw install from git repo may be needed and we need to ensure the CUDA extension is actually compiled and installed. Check whether this is correct below.\n",
        "- Set a memory limit to avoid OOM error.\n"
      ],
      "metadata": {
        "id": "ca-VSilecS-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your locale, should have UTF-8\n",
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "id": "U1Q5CpM6cqCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If not, use this to force override\n",
        "\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Uv6gvOUMczuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PanQiWei/AutoGPTQ.git\n",
        "!cd AutoGPTQ && BUILD_CUDA_EXT=1 pip install .\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "szi3LDcCc-Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AutoGPTQ\n",
        "\n",
        "# Check whether CUDA extension is installed correctly, will throw exception if not\n",
        "import autogptq_cuda"
      ],
      "metadata": {
        "id": "7U9Csay0eI0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With these out of the way, let's actually do it."
      ],
      "metadata": {
        "id": "kJg_F6eGgPjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b\", use_fast=False)\n",
        "\n",
        "# GPTQ quantization require a sample text, presumably to collect statistics on token distribution?\n",
        "test1 = \"\"\"Below is a conversation between user and assistent. The assistent is helpful and skillful.\n",
        "User: Hi! How are you today?\n",
        "Assistent: I'm feeling good! Anything I may help you with?\n",
        "User: Write a short essay that analyze ritual from a cultural and anthropology framework.\n",
        "Assistent:  Sure! Let's start with the definition of ritual. According to anthropologists, ritual is a set of actions that are performed in a prescribed manner, usually with the intention of achieving a desired outcome. Rituals are often associated with religious or spiritual practices, but they can also be used in non-religious contexts, such as weddings, funerals, or celebrations.\n",
        "In a cultural context, rituals are often seen as a way for people to connect with their traditions, beliefs, and values. They can help to reinforce social norms, transmit knowledge, and transmit values. Rituals can also serve as a way for people to express their identity and their place in the community.\n",
        "Anthropologists believe that rituals are a way for people to make sense of the world around them. They can help to make sense of the past, present, and future, and to make sense of the relationships between people and the natural world. Rituals can also help to create a sense of community and belonging, and to reinforce social norms and values.\n",
        "In conclusion, rituals are a powerful tool for people to connect with their traditions, beliefs, and values, and to make sense of the world around them. They can help to create a sense of community and belonging, and to reinforce social norms and values.\n",
        "User: Thanks. I heard that primitive tribes throughout the world have diversity in their rituals, and that some tribes are harsher than others in terms of the rite of passage for their members. Postulate some possible factors explaining this variance.\n",
        "Assistent:  Yes, primitive tribes throughout the world have a wide range of rituals, and some tribes are harsher than others in terms of the rite of passage for their members. Some of the possible factors that may explain this variance include the availability of resources, the level of social organization, and the level of technology.\n",
        "In societies where resources are scarce, it may be more important for people to have a clear and defined set of rituals that help to reinforce social norms and values. In societies with a high level of social organization, it may be more important for people to have a clear and defined set of rituals that help to reinforce social norms and values. In societies with a high level of technology, it may be more important for people to have a clear and defined set of rituals that help to reinforce social norms and values.\n",
        "In conclusion, the factors that may explain the variance in the rite of passage for members of primitive tribes throughout the world include the availability of resources, the level of social organization, and the level of technology.\"\"\"\n",
        "\n",
        "examples = [\n",
        "    tokenizer(test1)\n",
        "]\n",
        "\n",
        "# This config *how* would we do the quantization.\n",
        "# Check out r/LocalLlama community etc for tips on the best value to use\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,\n",
        "    group_size=128,\n",
        "    desc_act=False,\n",
        ")\n",
        "\n",
        "# Load model with memory limit\n",
        "model = AutoGPTQForCausalLM.from_pretrained(fp16_model_repo_id,\n",
        "                                            quantize_config,\n",
        "                                            max_memory={0:'14GiB', 'cpu': '10GiB'})\n"
      ],
      "metadata": {
        "id": "BRMbhXkmeJdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's the actual quantization:"
      ],
      "metadata": {
        "id": "ODAbD0qFi-hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.quantize(examples)"
      ],
      "metadata": {
        "id": "MW3_QYfUfAmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that, let's save the model similar to the `transformer` library."
      ],
      "metadata": {
        "id": "8DCOD6EYjCBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gptq_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-GPTQ\" # @param{type:\"string\"}\n",
        "gptq_model_save_dir = \"gptq-model\" # @param{type:\"string\"}\n",
        "\n",
        "model.push_to_hub(gptq_model_repo_id,\n",
        "                  save_dir=gptq_model_save_dir,\n",
        "                  use_safetensors=True)"
      ],
      "metadata": {
        "id": "fCfNcOlYfDbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart kernel again**"
      ],
      "metadata": {
        "id": "RgqplDVgjI3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-merged\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "f_0gbqR_vpiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GGML Quantization\n",
        "\n",
        "Finally, let's perform GGML quantization. It is a relatively standalone program with some helper python scripts that are needed only for the quantization task, so this should be more straight forward.\n",
        "\n",
        "First let's download *both* the merged model and the tokenizer. Note that the tokenizer is not specified in the model config and it is up to us to know what it is.\n",
        "\n",
        "Usually it'd be the tokenizer of the base model."
      ],
      "metadata": {
        "id": "jDbTsC8vjF_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/my_model/\" # @param{type:\"string\"}\n",
        "ggml_model_repo_id = \"lemonteaa/exercise-openllama-3b-qlora-axolotl-checkpoint200-GGML\" # @param{type:\"string\"}\n",
        "ggml_upload_filename = \"openllama-3b-qlora-axolotl-ck200.ggml.q4_0.bin\" # @param{type:\"string\"}\n"
      ],
      "metadata": {
        "id": "-9lPChAax-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "path = snapshot_download(fp16_model_repo_id, local_dir=base_dir) #local_dir=IMAGE_MODEL_DIR"
      ],
      "metadata": {
        "id": "D3yyKd8ujOSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "tok_path = hf_hub_download(repo_id=\"openlm-research/open_llama_3b\",\n",
        "                           filename=\"tokenizer.model\",\n",
        "                           local_dir=base_dir)\n",
        "\n",
        "!ls {base_dir}"
      ],
      "metadata": {
        "id": "l_uf3kxRwQ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's install `llama.cpp` again (notice the last line is needed as we're using their python scripts):"
      ],
      "metadata": {
        "id": "ehqueT4Mwg30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!make\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "5IuvrFzpwjp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time for the quantization. Fist convert to the `ggml` format:"
      ],
      "metadata": {
        "id": "qW287E-Vw6tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert.py {base_dir}"
      ],
      "metadata": {
        "id": "gUsXrYh6w-6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then quantize:"
      ],
      "metadata": {
        "id": "S2Y8zu-4xClL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./quantize {base_dir}/ggml-model-f16.bin {base_dir}/ggml-model-q4_0.bin q4_0"
      ],
      "metadata": {
        "id": "yVKitaBCxGyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lha {base_dir}"
      ],
      "metadata": {
        "id": "qMQYN9Wqx06l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last but not least, we can use the Huggingface hub SDK to upload files manually to the repo:"
      ],
      "metadata": {
        "id": "EhazDG9B0Vbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(ggml_model_repo_id, repo_type=\"model\")"
      ],
      "metadata": {
        "id": "GAzhY1DWyCbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj = os.path.join(base_dir, \"ggml-model-q4_0.bin\"),\n",
        "    path_in_repo = ggml_upload_filename,\n",
        "    repo_id = ggml_model_repo_id,\n",
        "    repo_type = \"model\"\n",
        ")"
      ],
      "metadata": {
        "id": "KIYhpmtAypLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Tidying up - Upload Model Card\n"
      ],
      "metadata": {
        "id": "vvju8y-A0mSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Jinja2"
      ],
      "metadata": {
        "id": "zRmBubFS2x33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"\" # @param{type:\"string\"}\n",
        "repo_id = \"\" # @param{type:\"string\"}\n",
        "\n",
        "desc_text = \"\"\"\n",
        "This is just the resulting sample artifact of an exercise running a tutorial colab/jupyter notebook.\n",
        "\n",
        "*Source:* [hkitsmallpotato/llm-collections](https://github.com/hkitsmallpotato/llm-collections)\n",
        "\n",
        "The following artifacts are possible:\n",
        "\n",
        "- Original QLora weights\n",
        "- Merged fp16/32 model\n",
        "  - Run using the `transformers` library\n",
        "- GPTQ 4bit quantized model\n",
        "  - Use `auto-gptq`, `exllama`, etc\n",
        "- GGML q4_0 quantized model\n",
        "  - Use `llama.cpp`\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "\n",
        "card_data = ModelCardData(language='en', license='mit')\n",
        "card = ModelCard.from_template(\n",
        "    card_data,\n",
        "    model_id = model_id,\n",
        "    model_description = desc_text,\n",
        "    finetuned_from = \"openlm-research/open_llama_3b\"\n",
        ")\n",
        "\n",
        "print(card)\n",
        "#card.push_to_hub(repo_id, create_pr=True)"
      ],
      "metadata": {
        "id": "VicZvpv40t2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}